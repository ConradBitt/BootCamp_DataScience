{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "preceding-irrigation",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Neste projeto será utilizada a base de dados fornecida pelo Hospital Sírio Libanês. Ela esta disponível no site do [Kaggle](https://www.kaggle.com/) na pagina do grupo do Sírio Libanês [COVID-19 - Clinical Data to assess diagnosis](https://www.kaggle.com/S%C3%ADrio-Libanes/covid19).\n",
    "\n",
    "Essa base de dados contém informações, não sensíveis, que diz respeito a quantidade de pacientes que foram ou não internados por covid-19 na clínica do hospital durante a pandemia de corona virus. A pandemia de COVID-19 atingiu o mundo inteiro, sobrecarregando os sistemas de saúde - despreparados para uma solicitação tão intensa e demorada de leitos de UTI, profissionais, equipamentos de proteção individual e recursos de saúde. O Brasil registrou o primeiro caso COVID-19 em 26 de fevereiro e atingiu a transmissão na comunidade em 20 de março. \n",
    "\n",
    "## O problema \n",
    "\n",
    "Apesar do virus não ter uma letalidade tão alta a disseminação dele é relativamente alta para os vírus respiratórios e ao infectar muitas pessoas, uma parcela delas precisará de assistência médica. Portanto, como a quantidade de pessoas que necessitam de assitência é extremamente alta ocorre o pressionamento do sistema de saúde.\n",
    "\n",
    "Há urgência na obtenção de dados precisos para melhor prever e preparar os sistemas de saúde e evitar colapsos, definido pela necessidade de leitos de UTI acima da capacidade, usando dados clínicos individuais - em vez de dados epidemiológicos e populacionais.\n",
    "\n",
    "O colapso do sistema de saúde ocorre quando toda a estrutura não comporta a quantidade de atendimentos de qualquer natureza devido a alta demanda dos serviços de saúde. Nesta situação não basta só ter equipes médicas, de enfermagem e outros técnicos, pois a estrutura não comporta.\n",
    "\n",
    "<img src='https://img.medscape.com/thumbnail_library/cdc_200313_flatten_the_curve_800x450.jpg'>\n",
    "\n",
    "\n",
    "## Objetivos \n",
    "\n",
    "Ilutrado os problemas acima, podemos definir como objetivos as seguintes tarefas.\n",
    "\n",
    "1. Prever a admissão na UTI de *casos confirmados* de COVID-19. Com base nos dados disponíveis, é viável prever quais pacientes precisarão de suporte em unidade de terapia intensiva?\n",
    "\n",
    "O objetivo é fornecer aos hospitais terciários e trimestrais a resposta mais precisa, de modo que os recursos da UTI possam ser arranjados ou a transferência do paciente agendada.\n",
    "\n",
    "2. Prever NÃO admissão à UTI de *casos COVID-19 confirmados*. Com base na subamostra de dados amplamente disponíveis, é viável prever quais pacientes precisarão de suporte de unidade de terapia intensiva?\n",
    "\n",
    "O objetivo é fornecer aos hospitais locais e temporários uma resposta boa o suficiente, para que os médicos de linha de frente possam dar alta com segurança e acompanhar remotamente esses pacientes. \n",
    "\n",
    "## O Conceito de Janela \n",
    "\n",
    "Na pagina do Kaggle é ilustrado o conceito de janela. Em uma janela existe dados agrupados de um paciente, que consiste em: \n",
    "\n",
    "**Paciente**: \n",
    " - Visita do Paciente\n",
    " - Agregada por janelas em ordem cronológica\n",
    "\n",
    "\n",
    "|Janela|Descrição|\n",
    "---|---\n",
    "0-2|de 0 a 2 horas da admissão no hospital\n",
    "2-4|de 2 a 4 horas da admissão no hospital\n",
    "4-6|de 4 a 6 horas da admissão no hospital\n",
    "6-12|de 6 a 12 horas da admissão no hospital\n",
    "Acima-12|Acima de 12 horas da admissão no hospital\n",
    "\n",
    "**OBS:** *Cuidado para NÃO usar os dados quando o paciente já estiver na UTI, pois a ordem do evento é desconhecida (talvez o evento \"coleta de informação\" tenha acontecido antes ou depois do paciente se encontrar na unidade). Eles foram mantidos para que no futuro possam aumentar este conjunto de dados em outros resultados posteriormente.*\n",
    "\n",
    "**Ilustração de como funciona uma janela de informações**:\n",
    "<img src='https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1591620%2Fb1bc424df771a4d2d3b3088606d083e6%2FTimeline%20Example%20Best.png?generation=1594740856017996&alt=media'>\n",
    "\n",
    "> Acima temos um paciente que deu entrada no sistema e só foi para a UTI na janela acima de 12h.\n",
    "\n",
    "<img src='https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1591620%2F77ca2b4635bc4dd7800e1c777fed9de1%2FTimeline%20Example%20No.png?generation=1594740873237462&alt=media'>\n",
    "\n",
    "> Acima temos um paciente que foi para a UTI na janela de 2h. \n",
    "\n",
    "Note que até o paciente ir para a UTI podemos utilizar os registros informados na base de dados, entrentanto se o paciente foi para UTI, as informações não podem ser usadas, pois não sabemos se ela foi feita antes do paciente ir para unidade ou depois. Então nesta situação as informações não tem poder preditivo.\n",
    "\n",
    "## Os dados \n",
    "\n",
    "Este conjunto de dados contém dados anônimos do Hospital Sírio-Libanês, de São Paulo e de Brasília. Todos os dados foram tornados anônimos seguindo as melhores práticas e recomendações internacionais.\n",
    "\n",
    "Os dados foram limpos e escalados por coluna de acordo com Min Max Scaler para caber entre -1 e 1. \n",
    "\n",
    "### A respeito da importância da anonimização dos dados\n",
    "\n",
    "A Lei Geral de Proteção de Dados Pessoais (LGPD), Lei nº 13.709, de 14 de agosto de 2018, dispõe sobre o tratamento de dados pessoais, inclusive nos meios digitais, por pessoa natural ou por pessoa jurídica de direito público ou privado, com o objetivo de proteger os direitos fundamentais de liberdade e de privacidade e o livre desenvolvimento da personalidade da pessoa natural. \n",
    "\n",
    "Tendo em vista a vigência dessa Lei, de suma importancia pra sociedade, é importante que os dados sejam anonimizados para respeitar os direitos dos cidadãos brasileiros. É importante citar isso pois anonimização de um dado deve ser muito bem feita afim de não violar o direito dos individuos.\n",
    "\n",
    "Note que só o fato de sabermos que a mostra foi retirada dentre os pacientes do Hospital Sírio Libanês em um intervalo X de tempo já reduz muito o fator de anonimidade do dado e se uma variável fosse explicitamente a comorbidade \"diabetes\", \"pressão alta\" ou \"HIV\", caso as variáveis não fossem *clusterizadas*, sabendo a prevalência de uma doença numa população é possível encontrar as características de cada amostra e é importante dificultar este tipo de análise para estar de acordo com a lei de proteção de dados.\n",
    "\n",
    "Uma forma de anonimizar esses dados é criar grupos de características entre as amostras evitando assim de informar exatamente quais são as características de um elemento da base de dados. \n",
    "\n",
    "fonte: [Lei Geral de Proteção de Dados - LGPD](https://www.planalto.gov.br/ccivil_03/_Ato2015-2018/2018/Lei/L13709.htm#ementa)\n",
    "\n",
    "\n",
    "### Dados Disponíveis\n",
    "\n",
    "|Grupo de dados| quantidade de informações|\n",
    "|---|---|\n",
    "|Informação demográfica| 3 |\n",
    "|Doenças pré-existentes| 9 |\n",
    "|Resultados do exame de sangue| 36 |\n",
    "|Sinais vitais | 6 |\n",
    "\n",
    "No total, temos 54 variáveis, expandidas quando *pertinente* para a **média, mediana, máximo, mínimo, diff (Máximo - Mínimo)** e **diff relativa (diff/mediana)**\n",
    "\n",
    "**Dicas**\n",
    "\n",
    "* **Dados Faltantes:**\n",
    "\n",
    "    **Problema:** um dos maiores desafios de trabalhar com dados de saúde é que a taxa de amostragem varia nas suas diferentes medidas. Por exemplo, sinais vitais são medidos com mais frequencia (normalmente de hora em hora) do que os exames de sangue (diários).\n",
    "\n",
    "    **Sugestão:** é razoável presumir que um paciente que não tem uma medida registrada em uma janela de tempo está clinicamente estável, apresentando  sinais vitais e exames de sangue similares aos das janelas vizinhas. Portanto, se pode preencher os dados faltantes usando os dados da janela anterior ou posterior. Atenção para a multicolinearidade e casos de variância 0 nestes dados quando for escolher seu algoritmo.\n",
    "\n",
    "* **Quanto antes, melhor!**\n",
    "\n",
    "    **Problema:** A identificação precoce destes pacientes que vão desenvolver um caso adverso da doença (e precisarão de cuidado intensivo) é chave para um tratamento apropriado (salvando vidas) e para administrar os leitos e recursos.\n",
    "\n",
    "    **Sugestão:** Enquanto um modelo preditivo que use todas as janelas de tempo vai provavelmente conseguir uma acurácia melhor, um bom modelo usando apenas a primeira janela (de 0 a 2 horas) é provavelmente mais clinicamente relevante. A criatividade é, no entanto, muito bem vinda, portanto sinta-se livre com a engenharia de variáveis e janelas de tempo. Atenção para medições repetidas nos indivíduos, uma vez  que estes valores estão altamente correlacionados quando explorarmos os dados.\n",
    "\n",
    "**Reconhecimento**\n",
    "A Sociedade Beneficiente de Senhoras Sírio-Libanês está comprometida com a ciência e a filantropia para que os casos tenham o melhor tratamento de saúde possível para aqueles que dele necessitam. Nós gostaríamos de agradecer especialmente nosso departamento jurídico, nosso Instituto de Pesquisa e Educação e o Squad de Arquitetura e Inteligência de dados Clínicos e Radiológicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-bacteria",
   "metadata": {},
   "source": [
    "## Critérios de avaliação do projeto \n",
    "\n",
    "Existem duas perspectivas que serão analisadas na aprovação do projeto exigidas pela Alura: \n",
    "\n",
    "* Técnico\n",
    "* Prático\n",
    "\n",
    "\n",
    "### Critério técnico\n",
    "\n",
    "#### Escopo do Projeto\n",
    "\n",
    "Delimitar qual será o escopo do seu projeto e colocá-lo, de fato, em prática pode ser bastante desafiador pois é um equilíbrio entre a criatividade/entusiasmo e o tempo.\n",
    "\n",
    "Por isso, começar, desenvolver e finalizar todas as frentes abertas em um estudo é valioso.Você, cientista, precisa mais uma vez, ponderar: explorar pouco as possibilidades e ter um estudo raso ou explorar muitas possibilidades e não ser capaz de fechar dentro do elemento limitador, o tempo.\n",
    "\n",
    "#### Estrutura do projeto\n",
    "\n",
    "É necessário que seu estudo seja bem organizado e estruturado, apresentando uma sequência lógica da análise.\n",
    "\n",
    "O projeto precisa expressar e justificar qual a linha de raciocínio foi criada e seguida durante o processo de elaboração.\n",
    "\n",
    "#### Storytelling e conclusões\n",
    "\n",
    "Parte da entrega de um estudo, é mostrar para a comunidade qual o seu valor, ou seja, contextualizar e trazer o interlocutor para o mesmo ponto de partida é vital.\n",
    "\n",
    "É imprescindível que você pense que seu interlocutor, muitas vezes, não sabe do que aquele estudo se trata e/ou nem tem familiaridade com tecnologia e programação. Por isso, o notebook precisa ser explicativo de forma que a informação seja acessível para todos.\n",
    "\n",
    "As conclusões parciais e a conclusão final são ótimos artifícios para que a informação você extraiu dos dados, seja mais facilmente entregue ao leitor (lembre-se: resultados podem ser inconclusivos, também).\n",
    "\n",
    "Lembrete: este projeto será apresentado, de maneira fictícia, para o gerente responsável pela modelagem de dados do time de Data Science do Hospital Sírio Libanês. Você precisará persuadi-lo de que seu modelo tem os pontos necessários para entrar em produção e ajudará a antever e evitar qualquer ruptura.\n",
    "\n",
    "#### Boas práticas de programação\n",
    "\n",
    "Parte essencial de Data Science é a construção de código fundamentada nas boas práticas de programação.\n",
    "\n",
    "Uma boa documentação do código, nomes significativos para as variáveis, a reutilização de funções, podem ser exemplos de como colocar esse conceito em prática.\n",
    "\n",
    "Por isso, durante a correção, será dada uma atenção especial a esse cuidado que deve ser dado ao notebook.\n",
    "\n",
    "#### Gráficos e Tabelas\n",
    "\n",
    "A organização dos dados em gráficos e/ou tabelas é fundamental para a construção de uma boa visualização dos dados, ou seja, precisamos entender como eles estão distribuídos e se comportam ao longo do tempo.\n",
    "\n",
    "Por isso, gráficos ou tabelas completos e viáveis são indispensáveis (ex: título explicativo, labels nomeadas. No caso específico de gráficos: escala ajustada, início em (0, 0) ou caso não aconteça, apresente justificativa, por exemplo).\n",
    "\n",
    "#### Pesquisas externas e cruzamento de dados\n",
    "\n",
    "Do ponto de vista do estudo, é muito enriquecedor que outras fontes de informações sejam usadas para agregar valor e corroborar na construção da argumentação do projeto. E do ponto de vista técnico, isso mostra adaptabilidade e pensamento sempre um passo à frente, isso porque o cruzamento de dados é um passo muito importante no seu amadurecimento enquanto Data Scientist.\n",
    "\n",
    "Porém, é preciso tomar bastante cuidado ao fazer essa junção: será avaliado o valor agregado à pesquisa, não somente o cruzamento em si.\n",
    "\n",
    "*Dica: os dados do DataSUS podem ser uma boa fonte de inspiração para os cruzamentos. Além disso, você pode expandir suas análises feitas durante os projetos do módulo 01 e módulo 04, visto que, a partir das nossas conclusões, conseguimos justificar a implantação de um modelo que visa monitorar a evolução do quadro pandêmico no Brasil que apresenta alta nos casos.*\n",
    "\n",
    "### Critério prático\n",
    "\n",
    "* Os critérios mínimos práticos são bastante objetivos e claros, cientista. Use como um lembrete sobre o conteúdo que deve produzir.\n",
    "\n",
    "* Os dados estão dentro do escopo? (É obrigatório o uso da base de dados da Covid-19, disponibilizada pelo Hospital Sírio Libanês - São Paulo e Brasília, no Kaggle)\n",
    "\n",
    "* Ao rodar o notebook, ele apresenta erros? (Warnings serão desconsiderados)\n",
    "\n",
    "* Quando necessário, as variáveis foram tratadas?\n",
    "\n",
    "* Se houve criação de variáveis, as mesmas foram descritas?\n",
    "\n",
    "* Ficou claro qual foi o modelo final escolhido e o que motivou a escolha?\n",
    "\n",
    "* Quais testes foram aplicados? Foi justificado?\n",
    "\n",
    "* O modelo foi testado e validado adequadamente?\n",
    "\n",
    "* O notebook tem uma narrativa convincente e coerente?\n",
    "\n",
    "* O projeto contém meios para visualizar dados (gráficos ou tabelas) que ajudam na argumentação dos pontos principais do cientista?\n",
    "\n",
    "* A bibliografia e fontes de dados alternativas foram citadas?\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-athens",
   "metadata": {},
   "source": [
    "# Importações e definições\n",
    "\n",
    "Aqui estão todos módulos python que vou utilizar neste trabalho, bem como as funções criadas para exibições. \n",
    "\n",
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos que é sempre bom ter à mão\n",
    "import numpy as np  # Versão: 1.19.2\n",
    "import pandas as pd  #  Versão: 1.2.2\n",
    "import seaborn as sns  #  Versão: 0.11.1\n",
    "from matplotlib import pyplot as plt  #  Versão: 3.3.4\n",
    "\n",
    "# statsmodels  Versão: 0.12.2\n",
    "from statsmodels import api as sm \n",
    "from statsmodels import stats as ss\n",
    "\n",
    "# Sci-kit Learn  Versão: 0.24.1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-native",
   "metadata": {},
   "source": [
    "### Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes = pd.read_excel('https://github.com/ConradBitt/BootCamp_DataScience/blob/master/ML%20em%20Saude/dados/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx?raw=true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-vegetation",
   "metadata": {},
   "source": [
    "## Definições \n",
    "\n",
    "Esta etapa pode não seguir a ordem cronológica do relatório, pos aqui ficam os códigos para evitar que tenha várias funções complexas ao longo da análise. Contruindo funções aqui facilita analisar apenas os outputs de cada função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.randint(0,2**32) # semente gerada: 3340758118\n",
    "seed = 3340758118\n",
    "\n",
    "# ===================================== Análise exploratória ======================\n",
    "# Estatisticas descritivas dos dados \n",
    "\n",
    "## Muda identificador \n",
    "def muda_identificador(dados):\n",
    "    \"\"\"\n",
    "    Muda o index para o identificador PATIENT_VISIT_IDENTIFIER\n",
    "    \"\"\"\n",
    "    dados_novo_id = dados.set_index('PATIENT_VISIT_IDENTIFIER')\n",
    "    return dados_novo_id\n",
    "\n",
    "## Listas de variáveis das estatsiticas fornecidas pelo Sírio Libanês\n",
    "def variaveis_com_media_mediana_max_min_diff_diffRel(dados):\n",
    "    \"\"\"\n",
    "    Retorna as colunas com as variaveis cuja as informações contém estatisticas\n",
    "    descritivas. Basicamente as colunas que tem rótulos do tipo '_MEAN','_DIFF'...\n",
    "    \"\"\"\n",
    "    media, mediana, maximo, minimo, diff, diff_rel, cat = [],[],[],[],[],[], []\n",
    "    for coluna in dados.columns:\n",
    "        if '_MEAN' in coluna: \n",
    "            media.append(coluna)\n",
    "        elif '_MEDIAN' in coluna: \n",
    "            mediana.append(coluna)\n",
    "        elif '_MAX' in coluna: \n",
    "            maximo.append(coluna)\n",
    "        elif '_MIN' in coluna: \n",
    "            minimo.append(coluna)\n",
    "        elif '_DIFF' in coluna and '_REL' not in coluna: \n",
    "            diff.append(coluna)\n",
    "        elif '_REL' in coluna:\n",
    "            diff_rel.append(coluna)\n",
    "        else:\n",
    "            cat.append(coluna)\n",
    "                \n",
    "            \n",
    "    return media, mediana, maximo, minimo, diff, diff_rel, cat\n",
    "        \n",
    "# Describe do statsmodels \n",
    "def descricoes_pandas_statsmodels(dados):\n",
    "    \"\"\"\n",
    "    Retorna uma tupla com estatisticas descritivas\n",
    "    usando o pandas e o statsmodels.\n",
    "    \"\"\"\n",
    "    descricoes_stats_models = ss.descriptivestats.Description(dados)\n",
    "    descricoes_pandas = dados.describe()\n",
    "    return descricoes_pandas, descricoes_stats_models\n",
    "\n",
    "def exibe_mapa_correlaçao(dados, title='', metodo = 'pearson', cmap='cool', figsize=(20,12), annot=False):\n",
    "    \"\"\"\n",
    "    Exibe um mapa de correlação.\n",
    "    \"\"\"\n",
    "    corr = dados.corr(method=metodo)\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(corr, cmap=cmap, annot=annot, fmt='.2f')\n",
    "    ax.set_title(title, fontsize=20, pad=20)\n",
    "    return ax\n",
    "\n",
    "# ===================================== Pré processamento ======================\n",
    "\n",
    "def categorico_para_quantitativo(dados, variavel_categorica):\n",
    "    \"\"\"\n",
    "    Muda os dados quantiativos para categoricos. \n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    le.fit(dados[variavel_categorica])\n",
    "    dados[variavel_categorica] = le.fit_transform(dados[variavel_categorica]) / 10\n",
    "    return dados\n",
    "\n",
    "\n",
    "#def categoriza_windon_pela_freq(window):\n",
    "#    \"\"\"\n",
    "#    Esta função categoriza a variável WINDOW pela frequencia \n",
    "#    da primeira janela. Basicamente extrai a frequência dos \n",
    "#    elementos que compõem a variável e divide cada uma delas\n",
    "#    pela frequencia do primeiro elemento. Em seguida faz um \n",
    "#    replace na variável original. \n",
    "#    \"\"\"\n",
    "#    frequencia_janelas = window.value_counts(normalize=True)\n",
    "#    frequencia_normalizada_1_janela = frequencia_janelas / frequencia_janelas[0]\n",
    "#    window = window.replace(frequencia_normalizada_1_janela)\n",
    "#    return window\n",
    "\n",
    "def categoriza_window_pela_frequencia(dados, variavel_categorica):\n",
    "    \n",
    "    frequencia_var_categorica = dados[variavel_categorica].value_counts(normalize=True)\n",
    "    frequencia_primeira_janela = frequencia_var_categorica[0]\n",
    "    mapa_frequencias_janelas = frequencia_var_categorica / frequencia_primeira_janela\n",
    "    dados[variavel_categorica] = dados[variavel_categorica].replace(mapa_frequencias_janelas)\n",
    "    return dados\n",
    "\n",
    "\n",
    "def interpolacao(grupo):\n",
    "    return grupo.interpolate(method='linear', limit=1, limit_direction='both')\n",
    "\n",
    "\n",
    "def preenche_nan(dados):\n",
    "    \"\"\"\n",
    "    Esta função preenche os dados do tipo NaN.\n",
    "    Primeiro ela agrupa os dados pelo identificador,\n",
    "    Aplica uma interpolação linear entre os visinhos\n",
    "    Aplica um preenchimento com o registro anterior\n",
    "    Aplica um preenchimento com o registro posteior\n",
    "    retorna o dataframe com os dados preenchidos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # separação de features em categoricas continuas e os classificadores.\n",
    "    features_categoricas = dados.iloc[:,:13]\n",
    "    colunas_features_continuas = dados.iloc[:,13:-2].columns\n",
    "    classificadores = dados.iloc[:,-2:]\n",
    "    \n",
    "    features_continuas = dados.groupby('PATIENT_VISIT_IDENTIFIER')[colunas_features_continuas].apply(interpolacao).fillna(method='backfill').fillna(method='ffill')\n",
    "    \n",
    "    dado_preenchido = pd.concat([features_categoricas, features_continuas, classificadores], axis=1, ignore_index=False)\n",
    "    dado_preenchido.columns = dados.columns\n",
    "    return dado_preenchido\n",
    "\n",
    "\n",
    "def foi_para_UTI(dado_agrupado : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Esta função verifica quais pacientes foram pra UTI na janela.\n",
    "    Se o paciente foi pra UTI na janela 2, então adiciona o valor 1\n",
    "    na coluna ICU anterior e retorna todo o grupo até ela.\n",
    "    \"\"\"\n",
    "    if np.any(dado_agrupado['ICU']):  # se o paciente foi em algum momento pra uti\n",
    "        \n",
    "        momento_foi_para_uti = len(dado_agrupado['ICU']) - dado_agrupado['ICU'].sum()  # momento em que foi para UTI\n",
    "        \n",
    "        for linha in range(len(dado_agrupado)):  # passar por todas as linhas\n",
    "            if dado_agrupado.loc[:,'ICU'].iloc[linha] == 1:\n",
    "                dado_agrupado.loc[:,'ICU'].iloc[linha - 1] = 1  # adicionar 1 na coluna ICU anterior\n",
    "        \n",
    "        if momento_foi_para_uti != 0:  #  se não foi na primeira janela\n",
    "            # retorna o grupo até a janela em que ele foi para UTI\n",
    "            return dado_agrupado.iloc[:momento_foi_para_uti, :]\n",
    "        #else:\n",
    "            # retorna o paciente que foi pra UTI na primeira janela\n",
    "            # talvez esta linha abaixo possa ser desconsiderada\n",
    "         #   return dado_agrupado.iloc[momento_foi_para_uti, :].to_frame().T   \n",
    "    else:\n",
    "        # retorna o paciente que não foi em nenhum momento para a UTI\n",
    "        return dado_agrupado\n",
    "    \n",
    "def filtro_janelas_uteis(dado):\n",
    "    \"\"\"\n",
    "    Filtra os registros pelas janelas úteis dos pacientes\n",
    "    que foram para a UTI e também daqueles que não foram.\n",
    "    \"\"\"\n",
    "    dado_filtrado = dado.groupby('PATIENT_VISIT_IDENTIFIER', as_index=False).apply(foi_para_UTI)\n",
    "    dado_filtrado = dado_filtrado.reset_index().set_index('PATIENT_VISIT_IDENTIFIER')\n",
    "    dado_filtrado = dado_filtrado[dado.columns]\n",
    "    return dado_filtrado\n",
    "\n",
    "\n",
    "def remove_variaveis_altamente_correlacionadas(dados, metodo_correlacao, valor_corte):\n",
    "    matriz_corr = dados.corr(method=metodo_correlacao).abs()\n",
    "    matriz_triangular_superior_booleana = np.triu(np.ones(matriz_corr.shape), k=1).astype('bool')\n",
    "    matriz_trig_sup_corr = matriz_corr.where(matriz_triangular_superior_booleana)\n",
    "    excluir_elemento = [coluna for coluna in matriz_trig_sup_corr.columns if any(matriz_trig_sup_corr[coluna] > valor_corte)]\n",
    "    \n",
    "    return dados.drop(excluir_elemento, axis='columns')\n",
    "\n",
    "# ===================================== Estimadores  ======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-banking",
   "metadata": {},
   "source": [
    "dummy_classifier = DummyClassifier()\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "logistic_regression = LogisticRegression()\n",
    "sgd_classifier = SGDClassifier()\n",
    "linear_svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-people",
   "metadata": {},
   "source": [
    "# Análise exploratória \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-mainstream",
   "metadata": {},
   "source": [
    "## Visualizando uma amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes.sample(10, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-attitude",
   "metadata": {},
   "source": [
    "## Visualização de variáveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-reminder",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sirio_libanes.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-nylon",
   "metadata": {},
   "source": [
    "### Comentário sobre análise das variáveis disponíveis \n",
    "\n",
    "> Verifica-se que tem um total de 1925 linhas com 231 variáveis. Entretanto como a variável `PATIENT_VISIT_IDENTIFIER` é uma variável identificadora, na realidade temos um total de 385 registros. de cada paciente com 230 variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes = muda_identificador(sirio_libanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-crown",
   "metadata": {},
   "source": [
    "> Dentre as variáveis, 255 são do tipo `float64`, 4 variáveis do tipo `int64` e 2 do tipo `object` que são do tipo string. \n",
    "\n",
    "> Outra informação que é relevante é a quantidade de variáveis do tipo **média, mediana, máximo, mínimo, diff (Máximo - Mínimo)** e **diff relativa (diff/mediana)**. Na pagina do Kaggle dizem que estas informações são pertinentes e por isso foram extraidas estas estatisticas. Foi criada listas com cada uma dessas colunas na parte de **Definições** do notebook para facilitar o trabalho com essas colunas específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "media, mediana, maximo, minimo, diff, diff_rel, categorias = variaveis_com_media_mediana_max_min_diff_diffRel(sirio_libanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-slave",
   "metadata": {},
   "source": [
    "> Além disso, outra variável importante é a `AGE_PERCENTIL`. Esta variável é do tipo `object`, que representa os percentis das idades que o paciente pertence. Por exemplo: \n",
    "\n",
    "* `AGE_PERCENTIL = 10th` idade de 0 até 10.\n",
    "\n",
    "* `AGE_PERCENTIL = 20th` idade de 10 até 20.\n",
    "\n",
    "*  `AGE_PERCENTIL = 30th` idade de 20 ...\n",
    "\n",
    "* $....$\n",
    "\n",
    "* `AGE_PERCENTIL = ABOVE 90th` idade de 90 até 100\n",
    "\n",
    "> Por mais que seja uma variável categórica, ela é ordenável. Seŕa feita uma modificação nesta variável na parte de pré processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-saturn",
   "metadata": {},
   "source": [
    "## Estatisticas Descritivas: Statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "descricoes_pandas, descricoes_stats_models = descricoes_pandas_statsmodels(sirio_libanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-commerce",
   "metadata": {},
   "source": [
    "Vou utlizar as estatisticas descritivas do [statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.stats.descriptivestats.Description.html#statsmodels.stats.descriptivestats.Description), apesar de que eu não vou comentar sobre todas elas acho relevante exibi-las tendo em vista que alguém pode se interessar por alguma estatsitica descritiva em específico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "descricoes_stats_models.numeric.round(13)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-defendant",
   "metadata": {},
   "source": [
    "### Comentário sobre estatisticas descritivas\n",
    "\n",
    "> Os dados foram agrupados pelos identificadores e cada identificador tem 5 registros, logo ao utilizar a função que gera as estatisticas, se os 1920 registros forem utilizados estaremos contabilizando 5 resultados do mesmo indivíduo, por isso estou divindo o resultado por 5, ou seja, essas estatisticas são um panorama dos grupos, mas não representam fielmente a característica independente de cada um. Além disso os dados foram arredondados na 13º casa decimal.\n",
    "\n",
    "> Como foi dito, algumas colunas não são numéricas, lógo não faz sentido extrair estatisticas descritivas delas por isso das 231 variaveis que tem a base de dados, apenas 228 foram utilizdas para a exibição.\n",
    "\n",
    "> Podemos verificar que pra cada paciente tem em média um dado faltante na linha `missing`\n",
    "\n",
    "> Um ponto interessante são as variáveis `DISEASE_GROUPING`, elas representam grupos de doenças, não sabemos exatamente quais são mas podemos verificar que dentre as 1920 informações a distribuição das médias de cada doença é diferente. Isso pode levar a seguinte hipótese \"*Podemos supor que das 385 informações 2% tem  `DISEASE_GROUPING_1`*\", **lembrando que as informações dos pacientes que foram para a UTI não tem relevancia preditiva devido ao anacronismo da coleta**, entretanto podemos utiliza-las para exibir informações de estatisticas descritivas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-chambers",
   "metadata": {},
   "source": [
    "### Descrição do pandas\n",
    "\n",
    "Caso esteja habituado com as estatisticas descritivas do módulo do pandas, vou deixar o output aqui para consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "descricoes_pandas.round(13)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-alcohol",
   "metadata": {},
   "source": [
    "## Correlação entre as variáveis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-longer",
   "metadata": {},
   "source": [
    "Vimos que muitas variáveis estão expressas em termo das medidas: **média, mediana, máximo, mínimo, diff (Máximo - Mínimo)** e **diff relativa (diff/mediana)**. Tendo esta informação dada na introdução, foi foi criada uma função chamada `variaveis_com_media_mediana_max_min_diff_diffRel()` para separar estas variáveis. A motivação desta separação é porque em muitos casos se a variável `diff` foi obtida atraǘes da diferenciação da variável `mean` a correlação entre as duas será alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-brain",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exibe_mapa_correlaçao(sirio_libanes,metodo='kendall',title='Mapa de Correlação entre todas as variáveis', annot=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-thailand",
   "metadata": {},
   "source": [
    "### Comentário sobre o mapa de correlação\n",
    "\n",
    "> Note que o mapa é literalmente uma matriz quadrada. Na diagonal principal temos correlação da variável com ela mesma, por isso a coloração rosa.\n",
    "\n",
    "> Entretanto no canto inferior direito temos um grupo rosa, ou seja essas variáveis entre si estão altamente correlacionadas, a variável `BLOODPRESSURE` com sufixos `MAX`, `DIFF` e `RELL` estão altamente correlacionada, é provável uma variável foi construida dependendo da outra isso faz com que a correlação seja alta. O mesmo acontece no canto superior esquerdo, a variável `BE_ARTERIAL` com sufixos `MAX`, `DIFF` e `RELL`.\n",
    "\n",
    "> Além dessas altas correlações esperadas das variáveis que representam diferentes estatisticas da mesma observação, temos por exemplo a coluna `INR_MEDIAN` com as linhas `TGO_MIN`, `TGP_MEAN` e `TTPA_MEDIAN` com uma correlação maior que 0.75.\n",
    "\n",
    "> O que pode ser feito para contornar este fato de features autamente correlacionadas é filtrar a correlação entre cada uma delas a partir de um valor de corte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-convert",
   "metadata": {},
   "source": [
    "# Pre processamento\n",
    "\n",
    "Nesta etapa vou modificar algumas variáveis afim de facilitar análises futuras. \n",
    "\n",
    "## Tratando `AGE_PERCENTIL`\n",
    "\n",
    "Como já foi dito anteriormente a variável `AGE_PERCENTIL` é uma variável do tipo `object`, entretanto se analisar os valores unicos dela é possível notar que são dados do tipo categóricos porem ordinais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes.AGE_PERCENTIL.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-burns",
   "metadata": {},
   "source": [
    "Tendo então as categorias vou utilizar uma função da biblioteca [Sci-Kit Learn] para fazer a transformação na variável, de um dado categórico para um dado quantitativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes = categorico_para_quantitativo(sirio_libanes, 'AGE_PERCENTIL')\n",
    "sirio_libanes.AGE_PERCENTIL.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-clark",
   "metadata": {},
   "source": [
    "Desta forma a transformação que foi feita é:\n",
    "\n",
    "| ANTES| DEPOIS|\n",
    "|---|---|\n",
    "|10th | 0 |\n",
    "|20th | 0.1 |\n",
    "|30th | 0.2 |\n",
    "|40th | 0.3 |\n",
    "|50th | 0.4 |\n",
    "|60th | 0.5 |\n",
    "|70th | 0.6 |\n",
    "|80th | 0.7 |\n",
    "|90th | 0.8 |\n",
    "|Above 90th | 0.9 |\n",
    "\n",
    "### Comentário sobre transformação do `AGE_PERCENTIL`\n",
    "\n",
    "> A escolha deste tipo de transformação é deixar os dados dentro do padrão das variáveis da base de dados. A maioria delas esta normalizada, ou seja, dentro do intervalo $[-1,1]$. Poderia deixa-la no intervalo $[0,9]$ como sugere o própio `LabelEncoder` do sci-kit learn, mas não segui este caminho por pura liberdade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-classification",
   "metadata": {},
   "source": [
    "## Tratando `WINDOW`\n",
    "\n",
    "Assim como foi transformada a variável categórica `AGE_PERCENTIL` em uma variável quantitativa, vou transformar a variável `WINDOW` em uma variável quantativa de acordo com a normaliação da primeira janela. Em outras palavras, eu vou calcular a frequência de cada janela em relação a base de dados e vou dividir cada frequencia pela frequencia da janela nº1.\n",
    "\n",
    "|nº |`WINDOW` | Frequência da Janela| Normalização pela 1º Janela |\n",
    "---|---|---|---|\n",
    "1|0-2 | 0.250|1.00\n",
    "2|2-4 | 0.231|0.924\n",
    "3|4-6 | 0.203|0.810\n",
    "4|6-12| 0.181|0.722\n",
    "5|ABOVE_12| 0.135|0.538\n",
    "\n",
    "A justificativa em fazer isso é porque estou assumindo que estar numa janela qualquer não é uma variável aleatória. A maior parte dos pacientes vai passar no minimo uma vêz pela primeira janela e como queremos classificar o quanto antes a situação do paciente, dar peso maior para as primeiras janelas é razoável e a forma que eu encontrei de fazer isso é normalizando em função da primeira janela.\n",
    "\n",
    "Além disso, a ideia de que essa variável transformada seja decrescente pode ajudar a criar uma dependência inversa com a variável `ICU`, pois quanto menor o valor da variável `WINDOW` mais tempo a pessoa passou no hospital e se isso acontece é de se esperar que a pessoa esta lá porque realmente precisa do atendimento. \n",
    "\n",
    "Uma outra possível abordagem é tentar utilizar uma distribuição de probabilidades geométrica para modelar esta transformação. O problema que eu tive nesta abordagem é a perda de memória da distribuição geométrica, isto é, a janela futura não depende da anterior e pro nosso problema em específico estar em uma ou outra janela não é um evento independente. Para mais informações sobre a distribuição de probabilidades geométrica consulte as notas de aula do curso [Introduction to Probability at anadvanced level](https://www.stat.berkeley.edu/~aditya/resources/AllLectures2018Fall201A.pdf) do professor Aditya Guntuboyina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes_teste = categoriza_window_pela_frequencia(sirio_libanes,'WINDOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes_teste.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-consideration",
   "metadata": {},
   "source": [
    "## Tratando registros com `NaN`\n",
    "\n",
    "Na etapa de estatisticas descritivas verificamos que em média, cada registro tem pelo menos 1 dado do tipo `missing`, digo isto baseado na descrição do statsmodels.\n",
    "\n",
    "Para tratar os dados faltantes vou seguir a sugestão encontrada no site do Kaggle, que é preencher com o dado anterior ou com o anterior. Este preenchimento considera que num curto espaço de tempo a variação dos dados não é tão abrupta.\n",
    "\n",
    "O processo de preenchimento sera o seguinte: \n",
    "\n",
    "1. Agrupar registros pelo index `PATIENT_VISIT_IDENTIFIER`.\n",
    "2. Realizar uma interpolação linear caso falte um entre dois dados.\n",
    "3. Preencher com o valor anterior.\n",
    "4. Preencher com o valor posterior.\n",
    "5. Petornar a base de dados com os dados preenchidos.\n",
    "\n",
    "Ao agrupar os dados pelo index eu tenho garantia que cada alteração será feita em cada grupo, evitando que o paciente indice 5 seja afetado pelo numero 4 ou 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-cuisine",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sirio_libanes = preenche_nan(sirio_libanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-storage",
   "metadata": {},
   "source": [
    "## Selecionando registros que podem ser usados para o modelo\n",
    "\n",
    "Como já foi dito na introdução do problema, podemos utilizar os dados dos registros cujo *o paciente ainda não foi para UTI*. Esta condição é imposta ao descrever o conceito de janela. \n",
    "\n",
    "<img src='https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1591620%2Fb1bc424df771a4d2d3b3088606d083e6%2FTimeline%20Example%20Best.png?generation=1594740856017996&alt=media' width=60%>\n",
    "\n",
    "> Note na ilustração acima que não podemos usar a janela das 12h tendo em vista que o paciente foi para UTI neste momento. \n",
    "\n",
    "<img src='https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1591620%2F77ca2b4635bc4dd7800e1c777fed9de1%2FTimeline%20Example%20No.png?generation=1594740873237462&alt=media' width=60%>\n",
    "\n",
    "> Outro ponto citado é que não podemos usar as janelas posteriores ao evento de ir para a UTI.\n",
    "\n",
    "A filtragem das janelas úteis que eu vou fazer consiste em **eliminar a janela cujo o paciente foi para a UTI**, além disso, acidionar $1$ na coluna `ICU` da janela anterior. Veja o exemplo na imagem abaixo:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/ConradBitt/BootCamp_DataScience/master/projeto_final/preprocessamento_filtro_janela.png'>\n",
    "\n",
    "> O paciente foi para a UTI na janela `ABOVE_12`, logo esta janela não pode ser utilizada, isso esta indicado no quadradinho vermelho. Portanto o que será feito é adicionar $1$ na coluna `ICU` da janela anterior (`6-12`) e utilizar todas essas janelas disponíveis como esta indicado no quadradinho verde.\n",
    "\n",
    "> **OBS**: *Verifique abaixo do quadradinho vermelho, a janela `0-2` o `ICU` desta linha é 1, ou seja, este paciente foi para a UTI na primeira janela e segundo a condição informada no Kaggle esta janela não pode ser utilizada e nem as posteriores. O que o filtro faz nesta situação é remover este paciente da base de dados.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes = filtro_janelas_uteis(sirio_libanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-planning",
   "metadata": {},
   "source": [
    "# Seleção de Modelos \n",
    "\n",
    "## O modelos escolhidos \n",
    "\n",
    "Para atingir o objetivo de estimar a necessidade ou não de um paciente precisar do serviço prestado pela unidade de terapia intensiva do hospital, precisamos não só de um modelo, mas de vários, a fim de comparar a qualidade de cada um deles. \n",
    "\n",
    "### Dummy Classifier \n",
    "\n",
    "A escolha de um modelo do tipo dummy se da por conta da necessidade de um *baseline*, isto é, se eu sortear uma pessoa aleatória em uma população e pedir pra ela classificar se um grupo de pessoas vai pra UTI ou não, a chance dessa pessoa classificar corretamente não será a melhor possível tendo em vista que ela foi sorteada aleatóriamente.\n",
    "\n",
    "O exemplo do que consiste um dummy comparado a uma pessoa aleatória, só funciona se essa pessoa sorteada aleatoriamente não for um funcionária da área de saúde ou médica intensivista (hahaha!) por mais que exista a probabilidade dessa pessoa ser sorteada, em geral a quantidade das pessoas com conhecimento técnico em saúde não é tão alta, logo a probabilidade de uma dessas pessoas ser sorteada ao acaso não é tão alta. Portanto, os modelos Dummies são responsáveis por fazer esse papel de \"*um chute aleatório*\" e portanto qualquer modelo para ser considerado melhor deve estar no mínimo acima da capacidade preditiva do dummy.  \n",
    "\n",
    "### Regressor Logístico \n",
    "\n",
    "A regressão logistica faz parte de uma categoria de modelos estatisticos chamados *Lineares generalizados*. Esta ampla classe inclui também regressão ordinária e *ANOVA*, além das regressões multivariádas, como *ANCOVA* e a regressão loglinear; uma excelente abordagem analítica dessas técnicas é feita por [Agresti(1996)](https://www.amazon.com/Categorical-Data-Analysis-Alan-Agresti/dp/0470463635) e uma abordagem mais prática é feita por [Gujarati(2011)](https://www.amazon.com.br/Econometria-B%C3%A1sica-Damodar-N-Gujarati/dp/8563308327) usado em alguns cursos de econometria. \n",
    "\n",
    "A regressão logistica em si é bastante utilizada em ciências médicas e sociais, pois é bem útil para a classificação de classes ou eventos mutuamente exclusivos, por exemplo: \n",
    "\n",
    "* Aprovação / Reprovação\n",
    "\n",
    "* Vitória / Derrota\n",
    "\n",
    "* Sadio / Doente\n",
    "\n",
    "Em geral problemas com essas características em que os resultados podem ser classificados em sucesso/fracasso ou $0$ e $1$, podem ser atacados com a regressão logistica, que apesar de ser um regressor se mostra muito útil em problemas cuja a resposta deve ser binária. É importante salientar que a regressão logistica não é a *função logistica*  apesar do modelo regressivo utiliza-la. \n",
    "\n",
    "\n",
    "### Classificador baseado em Máquinas de Vetores de Suporte \n",
    "\n",
    "O conceito do algorítmo de maquinas de vetores de suporte também podem resolver problemas de regressão ou classificação. Este algorítmo foi desenvolvido por Vladimir Vapnik e colaboradores na AT&T Bell Laboratories em 1997. São modelos mais rudimentares servem para resolver problemas de classificação linear binária e não probabilistica, entretanto, é possível utiliza-los em problema de várias saídas.\n",
    "\n",
    "A ideia dos algorítmos SVM (*Suport Vector Machine*) é que a cada nova entrada será classifica em relação à um hiperplano. Obviamente existem muitos hiperplanos que podem separar dois conjunto de dados e portanto o objetivo é que distância de um ponto gerado pelo dado de entrada até o hiperplano classificador seja a maior possível.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_0011.png' width=60%>\n",
    "\n",
    "Acima temos um exemplo de vários classificadores baseados em máquinas de vetores de suporte.\n",
    "\n",
    "### Classificação por Árvore de Decisão \n",
    "\n",
    "Árvores de decisão (*Decision Tree*) são métodos não paramétrico usado para classificação e regressão. O objetivo é criar um modelo que preveja o valor de uma variável de destino, aprendendo regras de decisão simples inferidas dos recursos de dados. Uma árvore pode ser vista como uma aproximação constante por partes. \n",
    "\n",
    "No exemplo abaixo, as árvores de decisão aprendem com os dados a aproximar uma curva senoidal com um conjunto de regras de decisão *if-then-else*. Quanto mais profunda a árvore, mais complexas são as regras de decisão e mais adequado é o modelo. \n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_001.png' width=60%>\n",
    "\n",
    "Árvores de decisão devem ser utilizadas com parcimônia, tendo em vista que, dependendo da profundidade da árvore pode ocorrer o fenômeno do *overfitting*. \n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "Se tratando de problemas de várias variáveis não basta uma derivada simples para buscar a situação de minimização dos resíduos, logo utiliza-se a função gradiente, que é uma derivada direcional que aponta sempre na direção onde ocorre a maior variação daquele conjuntos de variáveis. A sacada de analisar a direção contrária do gradiente é que ele vai apontar sempre na direção contrária à maior taxa de variação, isto é, na direção que minimiza.\n",
    "\n",
    "Stochastic Gradient Descent (SGD) é uma abordagem simples, mas muito eficiente, para ajustar classificadores lineares e regressores sob funções de perda, como as máquinas de vetores de suporte (linear) e a regressão logística já citada.\n",
    "\n",
    "Estritamente falando, SGD é apenas uma técnica de otimização e não corresponde a uma família específica de modelos de aprendizado de máquina. É apenas uma forma de treinar um modelo. Em geral consiste em minimizar a soma dos quadrados dos resíduos, assim como numa regressão comum. Acontece que não basta analisar apenas um gradiente de um grande conjunto de dados, a ideia então da técnica estocástica é dividir em lotes e ir computando o gradiente de pequenas amostras.\n",
    "\n",
    "Tendo em vista que SGD não é um modelo em si, mas sim uma técnica, o que se faz é utilizar os modelos anteriores mas com um ajuste efetuado pelo SGD. Em geral o estimador é o mesmo, o que muda é a técnica de otimização. Por exemplo, quando usamos o `SGDRegressor` do [Sci-Kit Learn](https://scikit-learn.org/stable/index.html) com o hiper parâmetro (`loss = 'squared_loss'` e `penalty = 'l2'`) o resultado é basicamente uma regressão linear, ou seja, um modelo equivalente mas que é ajustado via SGD em vez de ser ajustado puramente pela minimização dos resíduos. Da mesma forma o `SGDClassifier`(`loss = 'log'`) resulta numa regressão logistica com ajuste SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-array",
   "metadata": {},
   "source": [
    "# Extração de Variáveis \n",
    "\n",
    "Difinido os modelos que serão utilizados na estimação passamos para o próximo passo, a seleção de variáveis. Como uma das tarefas exigidas é estimar a necessidade, ou não, de um paciente precisar do serviço de UTI, precisamos selecionar algumas features (variáveis) relevantes para essa estimativa.\n",
    "\n",
    "Antes de selecionar as variáveis propriamente ditas, precisamos definir quais são nossas variáveis. Em geral um modelo pode ser expresso por uma função $f$, uma maquinha que vai receber insumos $x$ e vai retornar $y$. Matematicamente isso é expresso na forma: \n",
    "\n",
    "$$y = f (x),$$\n",
    "\n",
    "a variável $y$ é chamada de target, alguns autores chamam de classificador, na matemática esta é *a variável dependente* porque depende de $x$, no nosso caso $y$ é a coluna `ICU` que classifica se o paciente foi para a UTI ou não.\n",
    "\n",
    "A entidade $f$ é uma função, o nosso modelo propriamente dito, mas também pode ser um operador caso as variáveis sejam vetoriais ($\\vec{x} e \\vec{y}$), por fim, temos a variável $x$ chamada de variável independente que é basicante todas as outras colunas da base de dados (exceto `ICU`) que podemos utilizar para prever a necessidade do paciente no serviço da unidade de terapia intensiva.\n",
    "\n",
    "Como já foi visto na etapa de análise exploratória temos 230 variáveis. Logo nossa variável independente é um vetor $\\vec{x}$ onde cada componente deste vetor representa uma variável dos registros dos pacientes, isso significa que nosso problema pode ser representado matematicamente como: \n",
    "\n",
    "$$y = f(\\vec{x}), \\text{ onde }\\vec{x}=(x_1,x_2,...,x_{230}).$$\n",
    "\n",
    "Vamos definir nossas variáveis abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = sirio_libanes.drop(['WINDO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-restriction",
   "metadata": {},
   "source": [
    "## Selecionando variáveis com base nas estisticas descritivas \n",
    "\n",
    "Na etapa de análise exploratoria no topico visualização das variáveis foi descrito o rótulo e o tipo de cada variável. Muitas delas são dependentes uma da outra, por exemplo as variáveis com sufixo `_DIFF` e `_DIFF_REL`  onde `diff = (Máximo - Mínimo)` e `diff_rel = (diff/mediana)`. dito isso eu vou seprar a base de dados levando em conta esses rótulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes_media = sirio_libanes[media]\n",
    "sirio_libanes_mediana = sirio_libanes[mediana]\n",
    "sirio_libanes_max = sirio_libanes[maximo] \n",
    "sirio_libanes_min = sirio_libanes[minimo]\n",
    "sirio_libanes_diff = sirio_libanes[diff]\n",
    "sirio_libanes_diff_rel = sirio_libanes[diff_rel]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-champion",
   "metadata": {},
   "source": [
    "\n",
    "## Removendo variáveis altamente correlacionadas \n",
    "\n",
    "Na etapa de análise exploratória foi verificada a matriz de correlação. Nesta etapa vamos eliminar uma das variáveis que tem correlação acima do valor de corte `0.95` com outra. O motivo dessa remoção é tentar eliminar as variáveis que tem uma dependência intrínceca, por exemplo `_DIFF` e `DIFF_REL`. Posteriormente vamos testar se essa remoção teve algum ganho preditivo ou não. Além disso, diferente da etapa de análise exploratória, onde a correlação utilizada foi a de Pearson, aqui utilizaremos a correlação de Kendall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirio_libanes_filtrado_cc = remove_variaveis_altamente_correlacionadas(sirio_libanes, metodo_correlacao='kendall', valor_corte=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "exibe_mapa_correlaçao(sirio_libanes_filtrado_cc, title='Mapa correlação técnica de Kendal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-latitude",
   "metadata": {},
   "source": [
    "## Automatização na seleção de Features\n",
    "\n",
    "sklearn.feature_selection.RFECV(estimator, *, step=1, min_features_to_select=1, cv=None, scoring=None, verbose=0, n_jobs=None, importance_getter='auto')\n",
    "\n",
    "sklearn.feature_selection.SelectFromModel(estimator, *, threshold=None, prefit=False, norm_order=1, max_features=None, importance_getter='auto')\n",
    "\n",
    "sklearn.feature_selection.f_classif(X, y)\n",
    "\n",
    "sklearn.feature_selection.f_regression(X, y, *, center=True)\n",
    "\n",
    "sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, *, k=10)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection.feature_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-place",
   "metadata": {},
   "source": [
    "# Usar o conceito da estatisticas:\n",
    "\n",
    "> **Se são muitas variáveis, então todas são igualmente provaveis para um modelo, então portanto devemos fazer uma seleção aleatória (algum módulo do sklearn features selection) usando modelos com parâmetros estimados aleatóriamente RandomForest.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-school",
   "metadata": {},
   "source": [
    "# Referências \n",
    "\n",
    "\n",
    "[COVID-19 - Clinical Data to assess diagnosis](https://www.kaggle.com/S%C3%ADrio-Libanes/covid19)\n",
    "\n",
    "\n",
    "https://www.stat.berkeley.edu/~aditya/resources/AllLectures2018Fall201A.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-vatican",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
